\section{Virtual Kubelet}
Virtual Kubelet\cite{virtual} is an open source project by Microsoft that provides a programmable kubelet API interface to developers. It was developed to run workloads on Container as a service providers, such as Azure Container Instances. When started with a valid kubeconfig file, virtual kubelet registers itself as a node to the Kubernetes Cluster and executes commands given from the cluster. It has a programmable backend, called provider, through a Golang Interface for developers to write custom deployment logic to their own infrastructure. In Azure's case, using Virtual-Kubelet, they are registering their ACI service as a node with almost unlimited capacity, as ACI, in theory, can scale up number of containers without any restriction from the underlying hardware. There are also third-party providers, e.g. AWS fargate or Nomad, to offload workload to their respective container services. Microsoft also uses this techology to connect it's Kubernetes service with it's Azure IoT Hub service.\cite{Chandra2019} They are deploying docker containers to their edge devices through the kubernetes API. Virtual kubelet is started with the following command:

\begin{lstlisting}[language=python,caption={Command to run Virtual Kubelet},captionpos={b},label={lst:vkcommand}]
./virtual-kubelet --nodename $ID --provider unikernel \\
 --kubeconfig kubeconfig.yaml --labels $LABELS
\end{lstlisting}

In the command \textit{\$ID} is a random generated string to give as node name. As virtual-kubelet can work with different providers, name of the provider is also given as argument. The providers written for this thesis is called \textbf{unikernel} and will be explained more in detail. \textit{\$LABELS} are to set kubernetes node resource labels and it plays a big role on identifying different edge devices, thus multiple labels can be given.

While developing with virtual kubelet a side product of that effort was the ability to simulate large scale clusters within a reasonable budget. That part is explained in the next section.
\subsection{Simulating Large Scale Clusters}
The working principle of virtual kubelet is shown in figure \ref{fig:vk}. As virtual-kubelet is only a binary, it can run in a docker container and that docker container can be deployed to the same Kubernetes cluster and it uses the internal kubeconfig file to register itself as a node. In either case, those containers can be scaled to add new nodes to the kubernetes cluster without scaling up the underlying hardware node count. This creates a testbed for researchers to test new scheduling algorithms. As every node can be given a arbitrary resource value from outside, different resource optimization cases can be tested in an artificially created mega cluster. A pod deployed to a virtual node, does not need to run and can be shown as healthly when queried by the kubernetes master. The only workload that needs to be run is the virtual-kubelet binary, which is only takes 34.3 MB in an alpine image when compressed with golang strip flags and upx. A fairly powerful single node kubernetes cluster can run hundreds of it , and the user will see a 100+ node cluster.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/vk.png}
  \caption{Working principle of Virtual Kubelet \cite{virtual}} \label{fig:vk}
\end{figure}

Another option is to run those containers in a different virtual machine , which is not a node of the target kubernetes cluster. A valid kubeconfig can be used among running containers through a shared volume and a tool like docker-compose can be used to scale them. In this thesis, this approach was used for testing. A chunk of the docker-compose file can be seen in \ref{fig:docker-compose}.

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}[language=ruby]
    services:
    node-temp:
      build: .
      image: atakanyenel/vk-client
      deploy:
        replicas: 2
      volumes:
        - ${PWD}:/tmp
      environment:
        - LABELS=sensor=temperature
\end{lstlisting}
\end{tabular}
\caption{A virtual node deployment}\label{fig:docker-compose}
\end{figure}


A valid kubeconfig file can be found in the same directory as the docker-compose file and started containers use it to register themselves. When started, this configuration adds two additional nodes to the cluster with the label \textit{sensor=temperature}.

When creating virtual nodes, it's better to use docker containers , because virtual-kubelet binary listens on certain ports when running and it's hard to start two instances of it on the same machine. Docker containers provide that abstraction.

While testing for this thesis, edge devices were simulated on different virtual machines with different internet connections speeds. As it's impossible to test 100+ edge devices on the wild, nodes named Rpi-\{random-id\} were generated with different labels according to their simulated tasks.

%Todo : citation needed, maybe throttle
\subsection{The Unikernel Provider}
The only customizable part of virtual-kubelet is the provider interface. The interface consists of 15 methods that needs to be implemented by the provider developer. Most of the methods are regarding the lifecycle of a pod, while some of them are responsible of the health of the node. 

This thesis implements a single provider called \textit{unikernel} for the unikernel runtime. A single virtual-kubelet binary can support multiple providers through command line arguments as can be seen from listing \ref{lst:vkcommand}. In this project, unikernel deployment is happening in 3 different environment. It's possible to write different providers for different environments but to decrease the built binary size, a single provider was compiled using Golang's build tags for different environments. This requires the deployment medium to be known beforehand, instead of virtual-kubelet figuring it by itself by checking the installed hypervisor.

Current open source distribution of virtual-kubelet does not support giving labels to registered nodes. Label based scheduling is an easy way of deploying correct programs to edge devices, so virtual-kubelet source code had to be forked and modified to update node labels from outside with arguments.

The first job done by the provider is to register the resources to the kubernetes master as available. In the following code \ref{fig:runtime}, the provider constructor reads CPU and memory data from the device where virtual-kubelet is deployed. Kubernetes API uses standartised metrics for measuring CPU and memory, so read values are cast to string before being sent. There is also a third value called \textit{podCapacity}, which restricts the maximum number of pods that can be run on the device , independent from whether the device has enough resources or not.
% Golang Code block start
\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}[language=python]
import "runtime"
(...)
var m runtime.MemStats
runtime.ReadMemStats(&m)
cpu := strconv.Itoa(runtime.NumCPU()),
memory := strconv.Itoa(int(m.Sys/1024/1024)) + "Mi"

\end{lstlisting}
\end{tabular}
\caption{Getting Resource data}\label{fig:runtime}
\end{figure}
% Golang Code block end
% Todo  -> talk about the execute code

\subsection{Communication between cluster and device}
Once a virtual node is deployed to the cluster, it is crucial to restrict it to deployments it's supposed to handle. To achieve this, deployments regarding the virtaul kubelet are tagged with a special label. First the \textit{nodeSelector} array of the specification get the item \textit{type: virtual-kubelet}. With that annotation , no real node tries to handle the deployment. The second annotation is to get the correct provider. In theory, a cluster can have multiple virtual nodes with different providers suited for different workloads and a deployment has to go to it's respective provider. Adding a \textit{tolerations} flag allows to select a particular provider. A chunk of an example deployment file can be seen in \ref{fig:deployment}.

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}[language=python]
    (...)
    nodeSelector:
      type: virtual-kubelet
    tolerations:
    - key: "virtual-kubelet.io/provider"
      operator: "Equal"
      value: "unikernel"
      effect: "NoSchedule"
\end{lstlisting}
\end{tabular}
\caption{Node specific Deployment}\label{fig:deployment}
\end{figure}
\textit{NoSchedule} effect protects real deployments from being handled by a virtual node. A pod without the exact tolerations will not be deployed to the node. It's easy to see that all that annotations work both ways. To isolate normal docker based deployments from virtual nodes and to isolate unikernel based deployments from real nodes.

Another approach of working with unikernel specific deployments is to extend the Kubernetes API by defining a \textit{custom resource}. Custom resources are more handcrafted, because they only contain information given by the developer. Stated by the kubernetes API, "When you combine a custom resource with a custom controller, custom resources provide a true declarative API." Kubernetes has an Operator Pattern to extend the API in a more official way. In the first development iteration of the Operator Pattern, it turned out that the custom controller would only read data from the \textit{unikernel custom resource} and create a deployment configuration with the keys explained above and the information from the custom resource. Then, that deployment would be handled exactly the same, so it only saves the user from adding above mentioned keys but increases the complexity of the cluster greatly by adding a custom resource definition and a controller.

\subsection{Communication between kubernetes and hypervisor}
Once a unikernel specific deployment reaches virtual-kubelet,all of it's data can be read. Every method that handles the pod lifecycle has a pod object as argument. An example signature can be seen in the \ref{fig:signature}. The usability of this increases greatly when one realises that , this pod related information is not read or processed anywhere else other than the running node. It allows to modify values to specific keys without comprimising integrity of the system. In a valid pod deploymnet, the \textit{image} key is used to specify the name of the docker container that deployment runs. If that container is not found in the given registry, kubernetes returns an error saying that deployment failed and image couldn't be found. In the unikernel deployment, the image key is used to specify the name of the unikernel binary to run. That name can be handled in many ways. In earlier developments, docker containers simulating edge devices where created with multiple example unikernel binaries, and there were only couple of options to give to \textit{image} key. Now a unikernel registry exists behind a FTP server and given values are searched there first, if they don't exist on the edge device. This approach is similar to one used by docker based deployments.

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}[language=ruby]
    func (s *Provider) DeletePod(ctx context.Context, pod *v1.Pod) error
\end{lstlisting}
\end{tabular}
\caption{DeletePod function Signature}\label{fig:signature}
\end{figure}

\textit{CreatePod} method is responsible for creating new pods. Upon receiving a new pod and checking that pod is valid, this methods executes a shell command to talk with the underlying hardware. This shell command executes domain specific command with supplied parameters. The command runs in a subroutine, and status of the pod is updated as running. While specific commands will be explained more in the upcoming section and interesting takeaway of this section is how the system hdoes context management with those commands. These commands are all wrong running, and if they end up stopping for a reason, they have to be restarted by kubernetes. \textit{GetPods} method is periodically called by the kubernetes api and that method calls another shell command to checked started unikernels. That output is then parsed as a pod object and returned back to master. The pods are saved in a dictionary with their name as key, and their context of their respective command as value. Their command context allows the system to call the \textit{cancel} closure of a command if that pod needs to be deleted, or their command output is send to the \textit{GetLogs} command to follow command output. Golang is very helpful in that area ,because by design it has great mechanics for context aware concurrency.

\textit{nodeConditions} is also a periodically called function by the master and it can be configured to reflect changes on the host system. For example, if many unikernel images are running on device, there might be not enough memory to start more instances and that methods would change \textit{OutOfDisk} condition to true.